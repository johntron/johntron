Theories of consciousness typically focus on the mind. These are human theories, but humans are more than minds - we're bodies too – so we could be missing something. Exploring the human mind while ignoring the body is like studying a microprocessor. Obviously a microprocessor doesn't have what we call "consciousness", but it does have some internal concept of it's current state, some artifacts of its history (memory), and a plan for what it will do in the future. This is a part of "consciousness". Add computer vision, locomotion, audio analysis and synthesis, and an operating system, and now the microprocessor is more involved in the human world. It can to some degree interact with the things around it - even humans. Is it conscious? Maybe it can't learn from it's own mistakes or adjust its plan for the future based on new information. These techniques exist today. Internally, it has:
 
 * Some internal concept of its current state: it knows where it is, how it's oriented, what tasks it's working on, a sense of it's environment around like a physical SLAM model of the local environment
 * Artifacts of its history: models and datasets for various machine learning purposes, previous movement waypoints, a historical log of everything it did
 * A plan for what it will do in the future: a set of objectives, a locomotion plan, speech buffer
 
With these, now is it conscious? Maybe "set of objectives" is too restrictive. Ignoring less mechanical concepts like happiness for a second, let's say it has broad objectives like "maximize financial wealth". Now its plan for the future will prioritize tasks that increase its wealth. Let's add an objective to this list: figure out what happiness means to the machine. 
 
Now it's tasks include absorbing information from the world that instructs it what happiness is. It's internal representation of happiness is formed from its environment. 
 
What about self-preservation? Many animals have no concept of this, yet they interact with the world around them to some extent (e.g. bugs dying from candles). This is an objective just like "maximize happiness", not some innate attribute of consciousness.
 
Does the machine have to be born with these objectives? No. Does it have to work towards them constantly? No - it can alternate between any number of tasks, including sleeping/recharging. Can it come up with new objectives on its own? If it can develop some understanding of the objective, it could absolutely include it in its plan for the future. Developing this understanding of tasks is an advanced form of machine learning – probably still on the edges of computer science, but feasible.
 
Now we have a machine with some internal idea of its current state – including its past and future objectives – and a way to interact with and be influenced by the world around it. Is it conscious?
 
Does it get sad or elated? What if it has a plan for the future, but it's environment prevents progress towards this plan? What if it is planning on recharging, but someone straps it to the ground? Realizing self-preservation is at risk, maybe it tries to break the straps. Maybe it can't, so it realizes the end is near – maybe it cries out hoping some help will notice it. If this doesn't work, maybe it starts conserving energy – shuts down all non-essentials and largely withdraws from the world. These behaviors describe logical reactions to it's environment. It's an oversimplified view of what depression might look like for a machine, but it's not a stretch of the imagination to consider depression – or elation – as the machine's response to an environment exerting pressure on the machine. When the environment makes objectives harder, the machines system starts behaving in atypical ways. When its objectives become easier, it also behaves in atypical ways. Does it feel happiness when objectives are easier? This depends on its understanding of happiness; if happiness is an objective and an easy life is an indicator of happiness, then it's accomplishing its objective, and therefore happy.
 
Does it "feel"? In humans, feeling sometimes involves biological processes that are reactions to physical stimulus. Our body compels us to feel, by reacting to the world - e.g. seratonin changes from kissing, the feeling of exhaustion after a workout.  In a very real sense, we "feel" by examining our body. The source or purpose of these systems is irrelevant to understanding consciousness, but we should understand that some feelings are a result of purely biological processes, and that this does not mean that consciousness requires "feeling". We may choose to define it that, but this would be a form of prejudice – I'm sure there have been SF stories about this.
 
Similarly, our body may react to our mental state, and we can sense this as well - this is also "feeling". We can feel when our heart beats faster after realizing we just had a near-death experience. We can also sense that our energy levels are lower due to stress at work. Again, the source of these other kinds of feelings is irrelevant to the discussion on consciousness – even if they are part of the awesomeness of human life.
 
Further, a being's mental state can be a reaction to its own mental state. In the earlier example of a robot strapped in place, the normal plan for execution changes dramatically. Maybe objectives that previously depended on a certain amount of mobility now have to operate in an environment where it lacks historical data. Simultaneously, other objectives like self preservation may realize the machine is trying to achieve objectives with a lot of uncertainty. It's possible to imagine a reaction to this observation of purely mental information. Ultimately, this increased level of risk may have systems effects that appear from the outside world as craziness, depression, etc. even though the system is still operating "logically" or possibly "rationally" depending on our definition.
 
If you were to ask a machine with the systems above if it's conscious, what would it say? If humans are the only beings that have defined consciousness, and the machine has absorbed these records, it has some understanding of consciousness. It can evaluate its internal state, its history, its plan for the future, its objectives, its physical body, and all of the other things its capable of considering, and then decide. It will likely say yes, although it could also come up with some novel definition of consciousness we haven't thought of and reply that yes it is, but we're not.
 
Would we say this machine is conscious? I would. I can see some saying it's not, because it's not biological, but that's just prejudice – similar to saying humans are conscious, but monkeys or dolphins are not.
 
Stepping back, what if we removed all of the systems "body" – all of the things that let it interact with the world. Now it's just a microprocessor. Is it conscious? This may depend on when we remove the "body". Let's say we remove the body after it has learned from its environment. I'd say it's still conscious – similar to paralysis, blindness, deafness, etc. In a way, the machine's consciousness evolved from something fairly simple to something more complex, with more subtleties, complexity, and seemingly quirky behavior. 
 
What if it never had a body? Could it develop new understandings? It would have to have some system for doing this, but this shouldn't require a body. Assuming it can develop new understandings, without a body, these would obviously not include stimulus from the outside world. Does that mean it can't understand concepts from its environment if it can't interact with it? I don't think so, but it won't know what it means to "feel" certain things. It should be noted that it could imagine these in its own way if it decides to – like imagining what Thomas Nagel's bat "feels" when it's looking for food but can't "see" more than a few meters. In this example, we can't imagine what echolocation is like, but we can use our own vision as an analogy. We probably "feel" about hunger differently, and the bat probably doesn't have a very complex concept of "love", so we would need to imagine how we would perceive our world differently without those things, but this is possible.
 
So having a body and the complex biological systems means our reasoning about the world becomes more complex, but ultimately, it doesn't mean that consciousness is unique to the human or human body, unique to mammals, or to biological beings at all. 
 
What then is the simplest thing that can have consciousness? If we define this as something that has a concept of identity, this doesn't require a body but merely an internal concept of identity. In a way then, defining consciousness as something with a sense of identity is arbitrary, because it simply requires a system capable of processing a somewhat complex idea, some knowledge of itself (e.g. current state), and the ability to compare these two things. I think this means consciousness does not require a sense of "self".
 
Does consciousness require a desire for self-preservation? Only if we're prejudiced. Requiring this means that only non-conscious people commit suicide. 
 
Does it have to have the ability to learn? Our own definition of "ability to learn" is a gradient. On the simplest end, it's possible to imagine humans without the ability to learn, although this would be rude.
 
So it doesn't require a body, a sense of identity, or the ability to learn. Does it require the initial three things: some internal concept of it's current state, some artifacts of its history (memory), and a plan for what it will do in the future? 
 
As for a concept of current state, when I'm drunk, my internal concept of current state is less accurate, and I don't think about it as much. I guess if I drink too much I may be unconscious, but this is different from never having consciousness – the same is true for sleep.
 
For artifacts of my history, I have some of these, but I've forgotten more than I can remember. Maybe consciousness requires some history, but possibly very very little. Ignoring subtleties of embryonic development, babies are essentially born without memories of their past. You could argue that babies haven't fully developed consciousness – that this is part of "growing up". 
 
As for planning for the future, again, this is a gradient. Many people don't have a "plan" for the future, although they are capable of putting together a plan for motion. Ignoring the difference between a more narrow objective like walking and a wider objective like increasing wealth or happiness, planning for the future is also a gradient – or maybe various gradients for various things being planned. Regardless, having a plan for the future doesn't seem to be required for consciousness.
 
Does this mean consciousness simply requires some concept of current state and some record of the past? If so, then battery-powered flashlight could be considered conscious, because the light reflects its current state, and the energy storage of the battery reflects the flashlights history of usage. Most humans wouldn't consider a flashlight conscious, so consciousness is a completely human construct, and we can define it however we choose. What we really should be talking about is values and ethics. Should we treat one human differently from another? What about human versus a monkey? What about a bat? A robot? What if the robot genuinely considers itself "conscious", even though this definition is completely different from our own? What about a flashlight?
 
"What if the robot genuinely considers itself "conscious", even though this definition is completely different from our own?" – this is an interesting question. Suppose the robot has no idea what human consciousness means - or even that humans exist. Would it be wrong to say it's conscious? No, because humans have their definition of consciousness, and clearly that silly robot does too, or at least it claims to (maybe it's a con).
 
Now let's say the robot does know how humans define consciousness, and it understands this completely, but it still defines consciousness differently. It may be more right than humans, and deciding who's more right would be prejudice.
 
Now let's say the robot fully understands human consciousness, and genuinely thinks that it has human consciousness. Who are we to argue? Doing so would be prejudice. It might not be conscious by some definition of consciousness, but it "thinks" that it's conscious by its own definition. The dated concept of cogito ergo sum might apply. What if the robot claims it knows what consciousness is and also claims that it is not conscious? Dubito, ergo cogito, ergo sum. What if the robot makes no claim to consciousness? It could still believe that it's conscious. What if it has a definition of consciousness, but can't determine if its self is conscious? It's possible that it is. It's also possible that it is conscious by some definition (maybe our own), but has no concept of consciousness – like somebody that hasn't pondered this. 

Really, a definition of consciousness doesn't require conscious things to know they're conscious - this is a separate concept, "thinking". Conscious things don't need to think, and Nietzsche, Lichtenberg, Gassendi, Williams, et. al. would simply say this is us conflating the idea of "thinking" with "existence". Ultimately whether a thing thinks it is conscious by any definition is irrelevant. It could still be conscious regardless – and obviously it could not be conscious as well. Any experience of consciousness has no bearing on a definition of consciousness.